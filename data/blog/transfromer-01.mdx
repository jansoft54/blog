---
title: Introduction to Transformers - Part 01 - Implementation of a translation model
date: '2024-10-13'
tags: ['transformer', 'nlp', 'code']
draft: false
summary: A short introduction to the architecture behind transformers how they can be utilized in NLP tasks.
---

Transformers have revolutionized the field of natural language processing (NLP), becoming the foundation for state-of-the-art models like BERT, GPT, and T5. Introduced in the seminal paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Attention Is All You Need</a>, transformers replaced recurrent and convolutional architectures by leveraging a novel mechanism called **self-attention**.

At their core, transformers process entire input sequences simultaneously, allowing them to capture global relationships within the data efficiently. This architecture not only improves performance on complex NLP tasks like machine translation and text summarization but also scales effectively to massive datasets and models.

In this series, we will explore the key components of the transformer architecture, starting with its foundational building blocks and how they enable efficient learning from sequential data.

## Architecture Overview

Before explaining every bit of the transformer model in detail, we will take a quick step back and give a small Overview
of the steps we will cover. The below [image](#transformer) describes the whole model very accuarctly.

<img
  id="transformer"
  src="/static/images/blog/transformers00/transformer.png"
  alt="Description of the image"
  style={{ width: '40%', height: 'auto', 'padding-left': '0%' }}
/>
As the original transformer was designed as a sequence-to-sequence processing model, its primary
application in the original paper was for translation tasks. The model takes two input sequences: a
source sequence and a target sequence. While modern large language models (LLMs) have adapted and
expanded upon this architecture for various tasks, we will focus for now on the original application
of translation to understand the foundational principles. Later, we will explore how these
principles are adjusted to form the backbone of modern LLMs.

We can see that this model takes two inputs. One for the encoder, the other for the decoder. Before feeding this inputs into the
model we create tokenizations and then embeddings for each token. These embeddings are concatenated with what one calls _positional encoding_.
As we will see later, the positional encoding gives the model a sense of ordering inside of the token sequence.

After the inputs are embedded and positional encoding is added, they are fed into the model. In the **encoder**, the inputs are duplicated three times and passed through a **Multi-Head Attention** block. Simply put, this block calculates the relative importance of each token (or its embedding) in relation to the others in the sequence. This mechanism, known as **self-attention**, was the groundbreaking innovation that paved the way for the development of modern large language models (LLMs).

Following the attention block, the outputs are normalized using a **LayerNorm** to ensure they have a mean of zero. This normalization step enhances stability and convergence during training.

Once normalized, the outputs are passed into an **MLP (Multi-Layer Perceptron)**, which is a simple feed-forward neural network. The MLP applies additional transformations to the data, further refining the representations.

These processed outputs are then passed to another encoder block, and this process is repeated for a total of $$n$$ such blocks. The stacking of these blocks allows the model to capture increasingly complex patterns and relationships in the data.
The decoder block in a Transformer is composed of several key components, each designed to process the target sequence and incorporate context from the input sequence encoded by the encoder.

The process begins with a **masked multi-head self-attention layer**. This layer operates on the embedded tokens of the target sequence and applies a mask to ensure causality. Masking ensures that each token can only attend to itself and tokens that came before it, preventing the decoder from accessing information about future tokens during sequence generation.
This mechanism is crucial for autoregressive tasks like language generation, where the model predicts one token at a time.

The output of this masked self-attention layer is then passed into a second attention block. This second block is different from the first because it performs **cross-attention**. Instead of attending solely to the target sequence, it uses the outputs of the encoder for two of its inputs: the keys $$K$$ and values $$V$$. The queries $$Q$$, however, are derived from the output of the masked self-attention layer.
This setup allows the decoder to focus on relevant parts of the input sequence, using the context provided by the encoder's output. Importantly, this attention block is **not masked**, as it operates on the encoder outputs, which remain static and do not require causality constraints.
Finally, the output of the cross-attention block is passed through a feed-forward neural network, similar to the one used in the encoder. This feed-forward step refines the representation and prepares it for either the next decoder layer or the output layer, where probabilities over the vocabulary are computed.

Now, this provides a high-level and simplified overview of the Transformer architecture, which gave us a foundational understanding of how this model operates. Now, letâ€™s dive into each step of the architecture with **great detail** so we can implement the Transformer from scratch in our machine learning library of choice. For me, that's **PyTorch**.

## Tokenization & Embedding

Before a language model can operate on any kind of natural language, the text needs to be transformed into a form that a computer can understand. In the field of **Natural Language Processing (NLP)**, this process is known as **tokenization**.

Tokenization refers to the process of assigning a unique number (or token ID) to a unit of text. These units are typically referred to as **tokens**, and their definition can vary depending on the type of tokenizer used. However, in most cases, tokens are **larger than a single character but smaller than an entire word**.

For example:

- The word "unbelievable" might be split into sub-tokens: `"un"`, `"believe"`, and `"able"`.
- These sub-tokens are then mapped to unique numerical IDs.

Take the sentence `The dog jumped over the green door!` for example. The tokenizer of OpenAI for
their _GPT4o_ model, would make

<img
  id="token"
  src="/static/images/blog/transformers00/token.png"
  alt="Description of the image"
  style={{ width: '40%', height: 'auto', 'padding-left': '0%' }}
/>
out of this. The respective token IDs would be `[976, 6446, 48704, 1072, 290, 8851, 42166, 13]`.
Now that we have the token IDs, the next step is to map them into a **high-dimensional vector space**.
This is done by using the token IDs as indices into an **embedding matrix**, which is essentially an array of vectors.
The dimension of these vectors is referred to as **$$ d_{\text{model}} $$**, often called the embedding dimension.
## Positional encoding
In a Transformer, there is no inherent notion of sequence order because the architecture processes tokens in parallel.
To address this, we use **positional encoding**, which injects information about the position of each token in the sequence into its embedding.
This is achieved by adding a positional vector to the token embedding, where each position is represented using a combination of sine and cosine functions of different frequencies.
These encodings allow the model to understand the relative and absolute positions of tokens within the sequence, enabling it to capture sequential information without explicit recurrence.

Mathematically, the positional encoding for a position $$pos$$ and dimension $$i$$ is defined as:

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

To see how this works in practice, let's again consider the following sequence of tokens:
`[976, 6446, 48704, 1072, 290, 8851, 42166, 13]`.
For simplicity, we'll assume that $$ d\_{\text{model}} = 4 $$.
This means each token will be represented by a 4-dimensional embedding vector.
A matrix where each **row** corresponds to the $$ d*{\text{model}}$$-dimensional embedding of a token could look like this:

```python
import torch

# Example embedding matrix
embedding_matrix = torch.tensor([
    [0.12, 0.32, -0.54, 0.87],  # Embedding for token 976
    [0.05, 0.73, -0.12, 0.45],  # Embedding for token 6446
    [0.93, -0.34, 0.17, -0.87], # Embedding for token 48704
    [0.15, 0.22, -0.11, 0.34],  # Embedding for token 1072
    [0.29, -0.12, 0.54, -0.23], # Embedding for token 290
    [0.43, 0.56, -0.32, 0.12],  # Embedding for token 8851
    [0.91, -0.13, 0.38, -0.44], # Embedding for token 42166
    [0.07, 0.41, -0.25, 0.89]   # Embedding for token 13
])
```

We will then iterate through every position of the sequence and denote the current position as $$ pos $$.
For each position, we calculate the positional encoding vector and add it to the corresponding token embedding.
The calculation depends on whether the dimension index  $$i$$ is even or odd.

For an **even** dimension index $$ 2i $$, the positional encoding is calculated using $$PE_{(pos, 2i)}$$, otherwise $$PE_{(pos, 2i+1)}$$.
The resulting positional encoding is then added element-wise to the embedding vector of the token at position $$ pos $$.
This ensures that the embedding incorporates both semantic information (from the token embeddings) and positional information (from the positional encodings).

# Encoder

## Self-Attention

<img
  id="attention"
  src="/static/images/blog/transformers00/attention.png"
  alt="Description of the image"
  style={{ width: '80%', height: 'auto', 'padding-left': '10%' }}
/>
The most important concept to understand in the Transformer architecture is the so-called
**self-attention** mechanism. This mechanism establishes an "attendance" between tokens that are
likely related to each other. It helps the model understand dependencies and relationships across a
sequence of tokens.

Suppose we have a sequence of tokens of length $$seq\_len$$, and each token is represented by an embedding of size $$d\*{\text{model}} $$.
This gives us a matrix of shape $$(seq_len,d\_{\text{model}}) $$, where each row corresponds to the embedding of a token in the sequence.

To perform self-attention, we copy this matrix **three times** and create three different matrices:

- **Queries ($$ Q $$)**
- **Keys ($$ K $$)**
- **Values ($$ V $$)**

Mathematically, these are obtained by multiplying the original matrix $$X$$ (the token embeddings), of shape $$(seq\_len,d\_{\text{model}}) $$, with learned weight matrices:

$$
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
$$

The self-attention mechanism computes the relationship between tokens in a sequence using **Queries ( $$Q$$ )**, **Keys ($$K$$)**, and **Values ($$V$$)**.
The formula for self-attention is as follows:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_{\text{model}}}}\right)V
$$

<img
  id="token"
  src="/static/images/blog/transformers00/atttention_weights.jpeg"
  alt="Description of the image"
  style={{ width: '100%', height: 'auto', 'padding-left': '0%' }}
/>

The first step in the attention mechanism is the **matrix multiplication** $$QK^\top$$, which computes the relationships between query and key tokens.

Each row $$i$$ in the resulting matrix contains the dot product of the embedding of the $$i$$-th token (from $$Q$$) with the embeddings of all other tokens (from $$K$$). For example, the first row represents how the token "Your" relates to all other tokens in the sequence based on their embeddings.

After this, a **row-wise softmax operation** is applied. This converts the raw dot product scores into probabilities:

- Each value in a row is normalized to fall between $$0$$ and $$1$$.
- The values in each row sum to $$1$$, allowing us to interpret them as probabilities or percentages showing how much a **query** token attends to each **key** token.

To prevent large values from destabilizing the softmax function, the result of $$QK^\top$$ is divided by $$\sqrt{d_{\text{model}}}$$. This scaling is necessary because:

- The variance of the dot product increases with $$d_{\text{model}}$$, leading to a standard deviation of $$\sqrt{d_{\text{model}}} \cdot \sigma$$, where $$\sigma$$ is the standard deviation of $$Q$$ and $$K$$.
- Dividing by $$\sqrt{d_{\text{model}}}$$ ensures that the standard deviation of the scaled values remains consistent, stabilizing the softmax operation and improving training stability.

Once we have calculated the attention weights, which form a matrix of shape $$(seq\_len, seq\_len)$$, the next step is to multiply this matrix with $$V$$. The $$V$$ matrix, which represents the values, is of shape $$(seq\_len, d_{\text{model}})$$.

The result of this multiplication is a new matrix of shape $$(seq\_len, d_{\text{model}})$$, where each row corresponds to a weighted combination of the embedding vectors, determined by the attention weights.

To understand this in detail, consider an example:

- The entry $$11$$ (first row, first column) of the resulting matrix is computed as the **weighted sum** of the first entries of all embedding vectors (rows in $$V$$), where the weights are the attention values from the first token to all other tokens.
- More generally, for an entry $$ij$$ of the resulting matrix:
  - We **accumulate** all $$j$$-values (columns) of the embedding vectors, weighted by how much attention token $$i$$ pays to all other tokens.

Now intuitively:

- Each resulting row represents a "mix" of embedding vector values, where the mix is determined by the attention weights.
- The attention mechanism essentially highlights the most important parts of the embedding vectors for each token, allowing the model to focus on relevant context within the sequence.

This step combines the relationships (from the attention weights) with the actual content of the embedding vectors, producing an output that captures both context and meaning.

## Skip connections & Normalization

After the self-attention layer, we end up with a matrix of shape $$(seq\_len, d_{\text{model}})$$.
The next step is to add the **input** of the attention layer to its **output**, a process known as a **skip connection** (or residual connection).

Skip connections are introduced to address the **vanishing gradient problem**.
During backpropagation, gradients are repeatedly multiplied as they propagate backward through the network.
If the gradient values are less than 1, this repeated multiplication can lead to extremely small gradient values, effectively stalling learning in earlier layers.
Adding a skip connection introduces an **additive term** that ensures the flow of gradients is not entirely dependent on the transformations within the layer.
This mechanism helps preserve information from earlier layers and keep the gradient from underflowing during traing

This result is then passed to a **normalization layer** (commonly layer normalization) to ensure a mean of $$0$$ and unit standart deviation.
